<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title> Machine Learning Resources - </title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="style.css" type="text/css" />
</head>
<body>
<h1 id="machine-learning-resources">Machine Learning Resources</h1>
<h2 id="courses">Courses</h2>
<p>These two are probably the best introductory courses out there right now:</p>
<ul>
<li><p><a href="https://www.coursera.org/learn/machine-learning">Machine Learning de Andrew Ng, Stanford</a><br /> Best introductory machine learning course? Maybe a bit dated. (lots of github repos with solutions)</p></li>
<li><p><a href="http://cs231n.github.io/">Convolutional Neural Networks de Karpathy, Stanford</a><br /> Best introductory machine learning course? It says convolutional but the first half deals with machine learning. Really updated!. (lots of github repos with solutions) <a href="http://vision.stanford.edu/teaching/cs231n/index.html">link to stanford version of the course</a></p></li>
</ul>
<h2 id="books">Books</h2>
<h3 id="introductory-books">Introductory books</h3>
<ul>
<li><p><a href="https://www.amazon.com/Learning-From-Data-Yaser-Abu-Mostafa/dp/1600490069/ref=zg_bs_3887_15">Learning from Data, by Abu-Mostafa, 2012</a> Really short and to the point, a great intro to machine learning from the statistical learning theory perspective (specially good for svm), introducing basic concepts such as overfitting, testing/validation sets, cross validation, model selection, supervised vs non supervised, etc.</p></li>
<li><p><a href="http://www.inference.phy.cam.ac.uk/itprnn/book.html">Information theory, inference and Learning, by McKay, 2003</a> Great book for self study, information theory chapters can be skipped, it is a bit general but great for understanding probabilitic and bayesian models, although requires a bit of math saaviness.</p></li>
<li><p><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning, by Nielsen, 2015, (free online)</a>.<br />Best introduction by far to Neural Networks (feedforward + convolutional). Easy, free and short.</p></li>
<li><p><a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Probabilistic Programming &amp; Bayesian Methods for Hackers, various authors, 2015/2016, (free online, open source)</a> Bayesian and probabilitic models for programmers (easy math!).</p></li>
<li><p><a href="http://www.deeplearningbook.org/">Deep Learning Book, by Bengio, 2016, (free online)</a> This is not really &quot;introductory&quot; in the sense of being easy to follow or having few prerequisites, but it is a great intro if you want to work on improving current neural network models.</p></li>
</ul>
<h3 id="classical-reference-books">Classical reference books</h3>
<p>These books are soft of traditional, and aren't designed for self study, I'd advise you to use them as a reference. They are sorted from easy to difficult. Even though all books cover similar topics, they have different approaches:</p>
<ul>
<li>Pattern classification/recognition: more signal processing<br /></li>
<li>Statistical Learning/Inference/Probabilistic/Graphical Models: More bayesian/statistical models<br /></li>
<li><p>Machine Learning/Learning From data: a bit more agnostic, more &quot;pure learning&quot; algorithms</p></li>
<li><p><a href="https://www.amazon.com/gp/product/0070428077/ref=pd_sim_14_9?ie=UTF8&amp;psc=1&amp;refRID=XENB6TPJHVN3FWVVX337">Machine Learning, by Tom Mitchell</a> This is sort of THE classical textbook reference for machine learning stuff. I've read mixed opinions about self-study.</p></li>
<li><p><a href="http://store.elsevier.com/Pattern-Recognition/Sergios-Theodoridis/isbn-9781597492720/">Pattern Recognition, by Theodoridis, 2008</a> Similar to Bishop's</p></li>
<li><p><a href="https://www.amazon.com/gp/product/0387310738/ref=pd_sim_14_2?ie=UTF8&amp;psc=1&amp;-refRID=M6A1E6MTZ2SNK9DGY0CN">Pattern Recognition and Machine Learning, by Bishop</a> A sequel to Duda's book, a bit more updated and of similar difficulty.</p></li>
<li><p><a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html">Pattern Classification, by Duda and Hart, 2000</a> The classic pattern classification book. A bit dated right now but great for reference.</p></li>
<li><p><a href="https://www.cs.ubc.ca/~murphyk/MLbook/">Machine Learning: a Probabilistic Perspective, by Keving Murphy, 2014</a>. Reputedly difficult and not for self-study, but updated.</p></li>
<li><p><a href="http://pgm.stanford.edu/">Probabilistic graphical models, by Koller, 2009</a> Great book but a bit disorganized. Also difficult, not recommended for self-study.</p></li>
<li><p><a href="https://www.amazon.com/The-Elements-Statistical-Learning-Prediction/dp/0387848576/ref=zg_bs_3887_10">The elements of statistical Learning, by Hastie, 2001</a> Famous for being terse and difficult, not recommended for self-study.</p></li>
</ul>
<h2 id="writing-papers-stuff">Writing papers &amp; stuff</h2>
<ul>
<li><a href="http://www.nature.com/scitable/ebooks/english-communication-for-scientists-14053993/contents">English Communication for Scientists, Nature</a></li>
</ul>
<h2 id="talkspresentations">Talks/presentations</h2>
<ul>
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/giving-a-talk.pdf">Giving a Talk, Peyton Jones</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=sT_-owjKIbA">Giving a Talk, Peyton Jones (video)</a><br /></li>
<li><a href="http://users.ha.uth.gr/tgd/pt0501/09/Tufte.pdf">The cognitive style of powerpoint, Edward Tufte</a></li>
</ul>
<h2 id="blogs-forums">Blogs &amp; Forums</h2>
<ul>
<li><a href="https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/">Math &amp; Programming, Jeremy Kun</a><br /></li>
<li><a href="http://colah.github.io/">Colah's blog</a><br /></li>
<li><a href="http://stats.stackexchange.com/">Cross Validated Q&amp;A (stackoverflow for statistics)</a></li>
</ul>
<h2 id="podcasts">Podcasts</h2>
<ul>
<li><a href="http://www.thetalkingmachines.com/">The talking machines</a><br /></li>
<li><a href="http://lineardigressions.com/">Linear Digressions</a><br /></li>
<li><a href="http://dataskeptic.com/">The Data Skeptic</a><br /></li>
<li><a href="http://partiallyderivative.com/">Partially Derivative</a><br /></li>
<li><a href="http://www.learningmachines101.com/">Learning Machines 101</a></li>
</ul>
<h2 id="newsletters-mailing-lists">Newsletters &amp; Mailing Lists</h2>
<ul>
<li><a href="https://mailman.srv.cs.cmu.edu/mailman/listinfo/connectionists">Connectionists</a><br /></li>
<li><a href="http://web.engr.oregonstate.edu/~dambrobr/uai.html">Uncertainty in AI</a></li>
</ul>
<h2 id="starting-out">Starting out</h2>
<p>If you are starting out in machine learning, focusing on neural networks the recommended path to take would be:</p>
<ul>
<li><p>General knowledge (this can easily take 6 months or more)</p>
<ul>
<li>Suscribe to Connectionists and Uncertainty in AI<br /></li>
<li>Start listening to some podcasts, they are mostly introductory and enable you to quickly get a superficial knowledge of various subjects and get to know some research groups in ml.<br /></li>
<li>Take Andrew Ng and Karphaty's online courses (in that order). Do all the homework/quizzes.<br /></li>
<li>While doing Andrew Ng's course, read Learning from Data, by Abu-Mostafa.<br /></li>
<li>While doing Karpathy's course, read Neural Networks and Deep Learning, by Nielsen<br /></li>
</ul></li>
<li><p>Specific neural networks stuff</p>
<ul>
<li>Read Bengios Book on deep learning<br /></li>
<li>Learn how to use a deep learning framework such as Torch/Tensor Flow/Caffe/etc. It seems Keras (built on top of Tensor Flow) is a good choice.<br /></li>
<li>Take a course/read a book on bayesian inference/probabilitic models<br /></li>
<li>Take a problem with a few datasets (maybe from a kaggle competition) and a model a try to improve its performance.<br /></li>
<li>Checkout papers from NIPS (one of the best ml/neural nets conferences)</li>
</ul></li>
</ul>
</body>
</html>
